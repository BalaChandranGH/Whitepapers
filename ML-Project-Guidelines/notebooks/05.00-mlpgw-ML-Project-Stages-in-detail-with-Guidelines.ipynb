{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36f4fcb-9542-4ae6-9419-5afd2aae21d8",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-Book-Cover-Small.png\"><br>\n",
    "\n",
    "This notebook contains an excerpt from the **`Machine Learning Project Guidelines - For Beginners`** whitepaper written by *Balasubramanian Chandran*; the content is available [on GitHub](https://github.com/BalaChandranGH/Whitepapers/ML-Project-Guidelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b6351-c01f-4f7e-86c1-2e3a069e4027",
   "metadata": {},
   "source": [
    "<br>\n",
    "<!--NAVIGATION-->\n",
    "\n",
    "<[ [Machine Learning Project – Process Definition](04.00-mlpgw-Machine-Learning-Project–Process-Definition.ipynb) | [Contents and Acronyms](00.00-mlpgw-Contents-and-Acronyms.ipynb) | [Other Considerations - How to choose ML Algorithms](06.01-mlpgw-Other-Considerations-How-to-choose-ML-Algorithms.ipynb) ]>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7840258e-0f9b-4364-89f4-c64b325cd51a",
   "metadata": {},
   "source": [
    "# 5. ML Project - Stages in detail with Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886a76e-b7ed-4251-be90-60715813d045",
   "metadata": {},
   "source": [
    "# 5.1. Stage-1: Business Understanding\n",
    "During this stage, the ML Problem to be solved is clearly understood and documented here. It would be a good idea to describe the problem in both terms:\n",
    "* Problem statement in business terms (Business problem)\n",
    "* Problem statement in analytical terms (Converted analytics problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceea11c-303a-4895-967e-8fde3c268d63",
   "metadata": {},
   "source": [
    "# 5.2. Stage-2: Data Understanding\n",
    "It's assumed that the following are already made available by the DS team. EDA is the main focus of the ML team.\n",
    "* Data requirements definition identifying the following:\n",
    "  - Data sources (and they could be)\n",
    "      - In-house or external source\n",
    "      - APIs, XML feeds, CSVs, Excel files\n",
    "      - Data mining/scrapping from online\n",
    "  - Data pipelines (and they could be)\n",
    "      - Streaming vs Batch\n",
    "      - Ingestion frequency\n",
    "  - Data environments\n",
    "      - Small vs Medium vs Big data\n",
    "* Collected raw datasets (and they should be)\n",
    "  - Diverse\n",
    "  - Unbiased\n",
    "  - Abundant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c8cdc4-6207-4fd1-9e0d-0d4412f7f611",
   "metadata": {},
   "source": [
    "## 5.2.1. Exploratory Data Analysis (EDA)\n",
    "### 5.2.1.1. Objectives of EDA\n",
    "* To get an overview of the distribution of the dataset\n",
    "* Check for missing numerical values, outliers, or other anomalies in the dataset\n",
    "* Discover patterns and relationships between variables in the dataset\n",
    "* Check the underlying assumptions in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9ab45-9f21-4817-a669-2959d2f7a01d",
   "metadata": {},
   "source": [
    "## 5.2.2. Summary of EDA Techniques\n",
    "EDA techniques depend on the type of data and the objectives of the analysis. The following is a summary of useful EDA techniques:\n",
    "```\n",
    "Type of data                  EDA techniques\n",
    "---------------------------   ------------------------\n",
    "- Categorical                 - Descriptive statistics\n",
    "- Univariate discrete         - Barplot\n",
    "- Univariate continuous       - Line plot, Histogram\n",
    "- Bivariate continuous        - 2-D scatter plot\n",
    "- 2-D arrays                  - Heatmap\n",
    "- Multivariate distribution   - 3-D scatter plot\n",
    "- Multiple groups             - Boxplot\n",
    "```\n",
    "\n",
    "The following table summarizes the useful EDA techniques depending on the objective:\n",
    "```\n",
    "Objective                                        EDA techniques\n",
    "----------------------------------------------   ------------------------------------------------\n",
    "- Check the distribution of a variable           - Histogram\n",
    "- Find outliers                                  - Histogram, scatterplot, box-and-whisker plot\n",
    "- Quantify the relationship between variables    - 2-D scatter plot, covariance, and correlation\n",
    "- Visualize the relationship between variables   - Heatmap\n",
    "- Visualize high-dimensional data                - Principal component analysis, 3-D scatter plot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38748be7-6689-4460-8cf4-824da04a1541",
   "metadata": {},
   "source": [
    "## 5.2.3. Text EDA: Understand the data with Descriptive Statistics\n",
    "* Dimensions of the dataset\n",
    "* An initial look at the raw data (e.g., first 10 rows & last 10 rows)\n",
    "* Basic information of the dataset\n",
    "* Statistical summary of the dataset\n",
    "* Class distribution of the dataset\n",
    "* Explore NA / NULL values in the dataset\n",
    "* Explore duplicates in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c372eb-b28c-46a1-9de3-3726be0bf7cd",
   "metadata": {},
   "source": [
    "## 5.2.4. Visual EDA: Understand the data with Visualizations\n",
    "* Draw Univariate plots to better understand each attribute\n",
    "  - Box and Whisker plots\n",
    "  - Histograms\n",
    "  - Pie-charts or Bar-charts (horizontal or vertical) to understand the distributions of data in int or float data-type\n",
    "* Draw Multivariate plots to better understand the relationships between attributes\n",
    "  - Scatter Plot Matrix\n",
    "  - Correlation maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bf28ea-2f2e-4df5-91e1-6d94eb9897c5",
   "metadata": {},
   "source": [
    "# 5.3. Stage-3: Research\n",
    "Based on the type of the problem (regression, classification or clustering, etc.) do research on the available algorithms and mention here the list of ML algorithms to be used to build the models. The final model will be selected based on their performances.\n",
    "* List down the names of algorithms, classifiers, and types of algorithms (linear, non-linear, ensemble, etc.) \n",
    "* Identify the Evaluation metrics selected for the project with the reasons\n",
    "* An overall approach on how this project will be developed (similar to development methodology in traditional projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77168211-65de-45ce-a340-f61512a42d96",
   "metadata": {},
   "source": [
    "## 5.3.1. List of model evaluation metrics\n",
    "Performance/Evaluation Metric: \n",
    "* An evaluation metric is a way to quantify the performance of a predictive model\n",
    "* There is no \"one fits all\" evaluation metric\n",
    "* Get to know your data\n",
    "* Keep in mind the business objective of your ML problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c206695b-5a50-40fd-8855-faca37bd9385",
   "metadata": {},
   "source": [
    "Select one or more metrics based on the problem type and business priorities. Commonly used metrics are:\n",
    "![](figures/MLPG-ModelEvalMetrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975effd-237e-4dfe-af05-ec93c1b96a89",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "* CV is a cross-validation score and, for regression, the scorer can be anything such as ``R^2, MAE, MSE, RMSE, and RMSLE``\n",
    "* CV is a cross-validation score and, for classification, the scorer can be anything such as ``Accuracy, ROC-AUC, PR-AUC, Logloss``, etc.\n",
    "* The following are not metrics, but they help to gain insight into the type of errors a model is making\n",
    "  - ``Confusion matrix``\n",
    "  - ``Classification report (produces Precision, Recall, F1 scores)``\n",
    "* Algorithm ``run-time is also a metric``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1afd6e-010a-4f0d-9e6c-24b3ae27213b",
   "metadata": {},
   "source": [
    "# 5.4. Stage-4: Data Preprocessing\n",
    "* The success of the ML algorithms depends on the quality of the data and the data must be free from errors and discrepancies\n",
    "* It must adhere to a specific standard so that ML algorithms can accept them, but this does not happen in reality\n",
    "* In reality, the data is dirty, incomplete, noisy, and inconsistent\n",
    "* Incomplete data means it has missing values and lacks certain attributes\n",
    "* The data may be noisy as it contains errors and outliers and hence does not produce desired results\n",
    "* The data may be inconsistent as it contains discrepancies in data or duplicate data\n",
    "* ML practitioners take steps to transform the collected raw data and process them to meet the input requirements of model training and testing that are suitable for ML algorithms\n",
    "* It involves several steps for cleaning, transforming, normalizing, and standardizing data to remove all the inadequacies and irregularities in the data\n",
    "* These steps are collectively known as _**Data Preprocessing**_ (or) _**Data Wrangling**_ (or) _**Data Preparation**_ (or) _**Data Augmentation**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1b8856-db61-45af-afa3-a5c7a22e1d97",
   "metadata": {},
   "source": [
    "## 5.4.1. Data Preparation tasks\n",
    "* **Data Cleaning**: Identifying and correcting mistakes or errors in the data\n",
    "* **Feature Selection**: Identifying those input variables that are most relevant to the task\n",
    "* **Feature Engineering**: Deriving new variables from available data\n",
    "* **Dimensionality Reduction**: Creating compact projections of the data\n",
    "* **Split datasets for train-test**: Separating datasets into input (X) and output (y) components\n",
    "* **Data Transforms**: Changing the scale or distribution of variables\n",
    "* **Handling Imbalanced Classes**: Imbalanced classes arise when one set of classes (majority class) dominates over another (minority class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612616ab-d4f7-4924-8054-c220b2aaf531",
   "metadata": {},
   "source": [
    "### 5.4.1.1. Data Cleaning\n",
    "* _``The process of identifying, correcting mistakes/ errors/ incomplete/ inconsistent/ noisy data, and preparing the dataset for analysis``_\n",
    "* The real-world dataset never comes clean; it comes in a wide variety of shapes and formats\n",
    "\n",
    "**Tidy data**\n",
    "* Tidy data provides a standard way to organize data values within a dataset\n",
    "* There are three principles of tidy data and they are:\n",
    "  - Columns represent separate variables\n",
    "  - Rows represent individual observations\n",
    "  - Observational units form tables\n",
    "* Tidy data makes it easier to fix common data problems, so, we need to transform the untidy dataset into tidy data\n",
    "\n",
    "**Signs of Untidy dataset**\n",
    "* _``Missing numerical data``_: Either they need to be deleted or replaced with a suitable test statistic\n",
    "* _``Unexpected data values``_: Solve the mismatched data types of a column and data values\n",
    "* _``Inconsistent column names``_: Address the column names that contain inconsistent caps and bad characters\n",
    "* _``Outliers``_: Remove or replace them with suitable test statistic as they can skew the results\n",
    "* _``Duplicate rows and columns``_: Drop them as they can cause bias in the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f8d3f-5719-48c6-8173-dc57f2bcf538",
   "metadata": {},
   "source": [
    "#### 5.4.1.1.1. Basic data cleaning activities\n",
    "* Remove unwanted/duplicate columns/features/attributes\n",
    "* Remove unwanted/duplicate rows/samples/examples\n",
    "* Remove embedded characters that may cause data misalignment \n",
    "  - Eg., embedded tabs in a tab-separated data file, embedded new lines that may break records, etc\n",
    "* Identify the inconsistent values and bring them to a common standard of expression (eg., N.Y or NY into New York)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c356f-1fb1-4e83-a71a-780b51c2e546",
   "metadata": {},
   "source": [
    "#### 5.4.1.1.2. Outliers detection\n",
    "* Outliers are extreme values that will skew the results if not addressed properly\n",
    "* There are many ways to identify outliers. Following are some of them:\n",
    "  - Box and Whisker plots\n",
    "  - Calculate Z-Score using the formula given below to identify the outliers (data points that fall outside a threshold)\n",
    "    ```\n",
    "    Z-Score = (x - Mean) / SD\n",
    "    ```\n",
    "* A couple of ways to handle outliers are:\n",
    "  - Drop them\n",
    "  - Replace with a suitable test statistic such as mean/median/mode\n",
    "\n",
    "_**IMPORTANT NOTE:**_\n",
    "* _Do outliers removals (discarding the samples or replacing the outlier values with 'mean' or 'median', 'mode' value) on training & test datasets separately_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2527c8-393b-4f60-a8d8-b2ffba0eee33",
   "metadata": {},
   "source": [
    "#### 5.4.1.1.3. Handling Missing Numerical values\n",
    "* Care must be taken while dealing with missing numerical values\n",
    "* Need to first identify the reason for the missing numerical values\n",
    "* There are several methods to handle missing values and each method has its advantages and disadvantages\n",
    "* The choice of the method is subjective and depends on the nature of the data and the missing values\n",
    "* ``Mark/Encode`` missing numerical values with ``NaN``:\n",
    "  - Missing values are encoded in different ways and they can appear as ``NaN, NA, ?, 0, ‘xx’, -1, or \" \" (blank space)``\n",
    "  - Pandas always recognize missing values as ``NaN``\n",
    "  - So, we must first convert all the ``NA, ?, 0, xx, -1, or \" \" to NaN``\n",
    "  - If the missing values aren’t identified as ``NaN``, then we have to first convert or replace such ``non-NaN`` entry with a ``NaN``\n",
    "* ``Drop`` them using ``dropna()`` method\n",
    "  - This is the easiest method to handle missing values. In this method, we drop labels or columns from a dataset with missing values\n",
    "* ``Replace`` with a suitable test statistic (such as mean/median/mode/forward-fill/back-fill)\n",
    "* ``Impute``\n",
    "  - In this method, we replace the missing value with the mean value of the entire feature column\n",
    "\n",
    "_**NOTE**_: _If we choose to fill the missing values with a test statistic like ``mean``, then we should compute the ``mean`` value on the training set and use it to fill the missing values in the training set. Then we should save the ``mean`` value that we have computed. Later, we will replace missing values in the test set with the mean value to evaluate the system. This is to avoid DATA LEAKAGE._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018204b-2808-4592-a21b-49cd8b106ea6",
   "metadata": {},
   "source": [
    "#### 5.4.1.1.4. Handling Missing Categorical values\n",
    "* Care must be taken while dealing with missing categorical values\n",
    "* Need to first identify the reason for the missing categorical values\n",
    "* In addition to the numerical values, the real-world datasets contain categorical data as well\n",
    "* ML algorithms require that some input data must be in numerical format; only then do the algorithms work successfully on them\n",
    "* The categorical data must be converted into numbers before they are fed into an algorithm\n",
    "* Some of the methods to handle missing values of categorical variables are:\n",
    "  - ``Drop``, i.e., ignore the variables if it is not significant\n",
    "  - ``Replace` it with the most frequent value (mode)\n",
    "  - ``Treat the missing data as just another category``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cada29e-54fa-474a-a7c3-5e189b71b89b",
   "metadata": {},
   "source": [
    "### 5.4.1.2. Feature Selection\n",
    "* Feature selection (FS) refers to techniques for selecting a subset of input features that are most relevant to the target variable that is being predicted\n",
    "* FS is primarily focused on removing non-informative or redundant predictors from the model\n",
    "* FS is about identifying those input variables that are most relevant to the task\n",
    "* FS removes columns with duplicate data/empty/unwanted data\n",
    "* FS techniques are generally grouped into those that use the target variable (``supervised``) and those that do not (``unsupervised``)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a7ce0b-2cc7-430f-a783-74b4b28a0989",
   "metadata": {},
   "source": [
    "#### 5.4.1.2.1. The Goals of Feature Selection Techniques\n",
    "* To reduce the computational cost of modeling\n",
    "* To improve the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded02626-5ddb-42ca-9172-f218dfc4395b",
   "metadata": {},
   "source": [
    "#### 5.4.1.2.2. Overview of Feature Selection Techniques\n",
    "* **Feature Selection**: Select a subset of input features from the dataset\n",
    "  - **Unsupervised**: Do not use the target variable (e.g. remove redundant variables)\n",
    "  - **Supervised**: Use the target variable (e.g. remove irrelevant variables)\n",
    "  - Some models are naturally resistant to non-informative predictors. Tree- and rule-based models, MARS and the Lasso, for example, intrinsically conduct FS\n",
    "* **Dimensionality Reduction**: \n",
    "  - Project input data into a lower-dimensional feature space\n",
    "  - Dimensionality reduction (eg., PCA) is an alternative to FS rather than a type of feature selection\n",
    "\n",
    "_**NOTE**_: _Just like there is no best set of input variables or best ML algorithm, there is no best feature selection method. One must discover what works best for a specific problem._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514f196-2e27-4f28-88e4-87d0e67b08e6",
   "metadata": {},
   "source": [
    "#### 5.4.1.2.3. Feature Importance\n",
    "**Feature importance** (FI) refers to techniques that assign a score to input features based on how useful they are at predicting a target variable.\n",
    "\n",
    "**The Uses of Feature Importance:**\n",
    "* Better understanding the data – FI scores can provide insight into the dataset\n",
    "* Better understanding a model – FI scores can provide insight into the model\n",
    "* Reducing the number of input features – FI scores can be used to improve a predictive model\n",
    "* Many ways to calculate FI scores and many models can be used for this purpose. Commonly used are,\n",
    "  - Feature importance from model coefficients\n",
    "  - Feature importance from decision trees\n",
    "  - Feature importance from permutation testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489e5cd-cee3-4e48-8d01-32ff57f2ee3d",
   "metadata": {},
   "source": [
    "#### 5.4.1.2.4. Mutual Information (MI)\n",
    "* MI is a lot like correlation in that it measures a relationship between two quantities\n",
    "* MI describes relationships in terms of _``uncertainty``_\n",
    "* The MI between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other\n",
    "* The benefit of MI is that it can detect _``any kind of relationship``_, while correlation only detects _``linear relationships``_\n",
    "* MI is a great general-purpose metric and especially useful at the start of feature development when we might not know what model we’d like to use yet\n",
    "* ``Advantages:``\n",
    "  - Easy to use and interpret\n",
    "  - Computationally efficient\n",
    "  - Theoretically well-founded\n",
    "  - Resistant to overfitting\n",
    "  - Able to detect any kind of relationship\n",
    "\n",
    "_**NOTE**_: _The uncertainty is measured using a quantity from information theory known as \"entropy\". The entropy of a variable means roughly: \"how many yes-or-no questions you would need to describe an occurrence of that variable, on average.\" The more questions you have to ask, the more uncertain you must be about the variable. MI is how many questions you expect the feature to answer about the target._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b21ee8-c7e9-4d75-be14-3c0af0a3a70f",
   "metadata": {},
   "source": [
    "### 5.4.1.3. Feature Engineering\n",
    "    “Better features make better data and better data make better models”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c42c0ea-9543-49be-8a0c-a740fe05784d",
   "metadata": {},
   "source": [
    "#### 5.4.1.3.1. The Goal and a Guiding principle\n",
    "* The goal of Feature Engineering is to make data better suited to the problem to be solved\n",
    "* Establish a baseline (score) by training the model on the un-augmented dataset. This will help us determine whether the new features are useful/worth keeping or discard them and try something else"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba95addb-168b-4210-a6b5-673fdc6e8882",
   "metadata": {},
   "source": [
    "#### 5.4.1.3.2. Creating features\n",
    "* Tips on discovering new features:\n",
    "  - Understand the existing features in the dataset by referring to the “data description document”\n",
    "  - Research the problem domain to acquire **domain knowledge**\n",
    "  - Study previous work. Solution write-ups are a great resource\n",
    "  - Use data visualization\n",
    "* Combine/aggregate existing features to create a new one (e.g., Total or Count or Mean or Ratio)\n",
    "* Reorder columns, if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983cd57e-12c0-4a36-a926-2c364102076b",
   "metadata": {},
   "source": [
    "#### 5.4.1.3.3. Tips on creating new features\n",
    "Keep in mind the model's strengths and weaknesses when creating features. Here are some guidelines: \n",
    "* Linear models learn sums and differences naturally, but can't learn anything more complex\n",
    "* Ratios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains\n",
    "* Linear models and NNs generally do better with normalized features. NNs especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so\n",
    "* Tree models can learn to approximate almost any combination of features, but when a combination is especially important, they can still benefit from having it explicitly created, especially when data is limited\n",
    "* Counts are especially helpful for tree models since these models don't have a natural way of aggregating information across many features at once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c6e56-344e-4e9e-8536-53706b388bcc",
   "metadata": {},
   "source": [
    "#### 5.4.1.3.4. Clustering with K-Means\n",
    "* We can use clustering algorithms (such as K-Means) in feature engineering. For example, we could attempt to discover groups of customers representing a market segment, for instance, or geographic areas that share similar weather patterns. Adding a feature of cluster labels can help machine learning models untangle complicated relationships of space or proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5733c5-d525-4db0-b7d4-994386782ba8",
   "metadata": {},
   "source": [
    "#### 5.4.1.3.5. Principal Component Analysis (PCA)\n",
    "* Just like clustering is a partitioning of the dataset based on proximity, we could think of PCA as a partitioning of the variation in the data\n",
    "* PCA is a great tool to help discover important relationships in the data and can also be used to create more informative features\n",
    "* **NOTE**: PCA is typically applied to standardized data. With standardized data \"variation\" means \"correlation\". With unstandardized data \"variation\" means \"covariance\". \n",
    "* There are two ways you could use PCA for feature engineering:\n",
    "  - The **first way** is to use it as a **descriptive technique**. Since the components tell us about the variation, we could compute the MI scores for the components and see what kind of variation is most predictive of our target. That could give us ideas for kinds of features to create: \n",
    "     - A product of ``'Height'`` and ``'Diameter'`` if ``'Size'`` is important, say, or a ratio of ``'Height'`` and ``'Diameter'`` if ``'Shape'`` is important. You could even try clustering on one or more of the high-scoring components\n",
    "  - The **second way** is to use the **components themselves as features**. Because the components expose the variational structure of the data directly, they can often be more informative than the original features\n",
    "* PCA Best Practices:\n",
    "  - PCA only works with numeric features, like continuous quantities or counts\n",
    "  - PCA is sensitive to scale. It's good practice to standardize the data before applying PCA unless we know we have a good reason not to\n",
    "  - Consider removing or constraining outliers, since they can have an undue influence on the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c076c-c5a6-4023-84fa-f478c98cbeaa",
   "metadata": {},
   "source": [
    "#### 5.4.1.3.6. Target Encoding\n",
    "All the techniques discussed above so far are for numerical features. The technique used to encode categorical features is called “target encoding”\n",
    "* A target encoding is any kind of encoding that replaces a feature's categories with some number derived from the target\n",
    "* Target encoding is sometimes called _mean encoding_ or _likelihood encoding_ or _impact encoding_ or _leave-one-out encoding_ or _binary encoding_ (if applied to a binary target)\n",
    "* Use cases for Target Encoding:\n",
    "  - **High-cardinality features**: A feature with a large number of categories can be troublesome to encode: a one-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target\n",
    "  - **Domain-motivated features**: From prior experience, we might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature's true informativeness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9592533-5d3b-4806-ba78-931e0fb42238",
   "metadata": {},
   "source": [
    "### 5.4.1.4. Dimensionality Reduction\n",
    "* Creating compact projections of the data\n",
    "* Dimensionality reduction can be considered as part of Feature Selection as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc2046a-f90e-4dd7-93c3-1df472976321",
   "metadata": {},
   "source": [
    "### 5.4.1.5. Split datasets for train-test\n",
    "* Separate datasets into Input (X) and output (y) components, if needed\n",
    "* Split the preprocessed datasets into _``train``_ and _``test``_ datasets, for model development, training, and evaluations\n",
    "  - _``train-test-split``_: \n",
    "    - A procedure to evaluate the performance of models on a large dataset (less computational cost)\n",
    "    - The dataset split can be done during the Preprocessing stage and the train & test datasets should be used at later stages\n",
    "    - Use _``train-test-split()``_ function from sklearn.model_selection\n",
    "    - k is internally set to 2 for a single split (1 train and 1 test dataset)\n",
    "  - _``LOOCV (Leave-One-Out-Cross-Validation)``_:\n",
    "    - This is another extreme to train-test-split where k is set to the total number of observations (k=n) such that each observation is given a chance to be held out of the dataset\n",
    "    - Use _``LeaveOneOut()``_ class from sklearn.model_selection\n",
    "* Remove outliers (replace with a suitable test statistic mean/median/mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f8ba1-c854-4aea-a275-20af3a25e792",
   "metadata": {},
   "source": [
    "### 5.4.1.6. Data Transforms\n",
    "* Changing the scale or distribution of variables.\n",
    "* Data transformation can be considered as part of Feature engineering as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f2aa9-5bf0-4b5f-ac53-91213e1ca5e0",
   "metadata": {},
   "source": [
    "#### **5.4.1.6.1. Numerical type**\n",
    "#### **Change scale**\n",
    "\n",
    "#### 5.4.1.6.1.1. Normalization a.k.a. Feature Scaling\n",
    "* Rescales numerical values to a specific range of 0-1 to reduce skews\n",
    "* Feature Scaling is a process used to normalize the range of independent variables so that they can be mapped onto the same scale\n",
    "* Exceptions:\n",
    "  - ``Decision trees`` and ``random forests`` are scale-invariant algorithms where we don’t need to worry about feature scaling\n",
    "  - Similarly, ``Naive Bayes`` and ``Linear Discriminant Analysis`` are not affected by feature scaling\n",
    "  - In Short, ``any Algorithm which is not distance-based is not affected by feature scaling``\n",
    "* Some of the common methods are:\n",
    "  - _``Log``_ transform\n",
    "  - _``MinMaxScaler``_ transform\n",
    "  - _``MaxAbsScaler``_ transform\n",
    "  - _``Normalizer``_ transform\n",
    "  - _``Binarizer``_ transform\n",
    "  - _``Decimal``_ scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32026d2b-af24-4425-bdf8-2de5d3584f7c",
   "metadata": {},
   "source": [
    "#### 5.4.1.6.1.2. Standardization (_``StandardScaler``_)\n",
    "* Standardize numerical data using the scale and center options (i.e., Mean of 0 and SD of 1))\n",
    "* It can be more useful for many ML algorithms, especially for optimization algorithms such as gradient descent\n",
    "* In standardization, first, we determine the distribution mean and standard deviation for each feature\n",
    "* Next, we subtract the mean from each feature, then we divide the values of each feature by its standard deviation\n",
    "* So, in standardization, we center the feature columns at mean 0 with a standard deviation of 1 so that the feature columns take the form of a normal distribution, which makes it easier to learn the weights\n",
    "\n",
    "_**NOTE**_: _Again, we should fit the _``StandardScaler``_ class only once on the training data set and use those parameters to transform the test set or new data set. This is to avoid DATA LEAKAGE._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592a32e-382e-41e2-8cd3-15d7fb9a5052",
   "metadata": {},
   "source": [
    "#### 5.4.1.6.1.3. Robust (_``RobustScaler``_)\n",
    "* _``StandardScaler``_ can often give misleading results when the data contain outliers\n",
    "* Outliers can often influence the sample mean and variance and hence give misleading results\n",
    "* In such cases, it is better to use a scalar that is robust against outliers\n",
    "* The _``RobustScaler``_ is very similar to _``MinMaxScaler``_. The difference lies in the parameters used for scaling. While _``MinMaxScaler``_ uses the minimum and maximum values for rescaling, _``RobustScaler``_ uses the interquartile (IQR) range for the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715c55e-192a-4227-af5e-c8de4db7f9bf",
   "metadata": {},
   "source": [
    "#### **Change distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19559a67-531b-4665-b032-bb049b51bf4f",
   "metadata": {},
   "source": [
    "#### 5.4.1.6.1.4. Power (_``PowerTransformer``_)\n",
    "* A power transform will make the probability distribution of a variable more Gaussian distribution\n",
    "* This power transform is available in the scikit-learn Python machine learning library via the _``PowerTransformer``_ class with the methods:\n",
    "  - _``Box-Cox``_ transform: Automatic power transform\n",
    "  - _``Yeo-Johnson``_ transform: Automatic power transform \n",
    "\n",
    "#### 5.4.1.6.1.5. Quantile (_``QuantileTransformer``_)\n",
    "* Numerical input variables may have a highly skewed or non-standard distribution\n",
    "* The quantile transform provides an automatic way to transform a numeric input variable to have a different data distribution, which in turn, can be used as input to a predictive model\n",
    "* A quantile transform will map a variable’s probability distribution to another probability distribution\n",
    "* The quantile function ranks or smooths out the relationship between observations and can be mapped onto other distributions, such as the uniform or normal distribution\n",
    "\n",
    "#### 5.4.1.6.1.6. Discretize (_``KBinsDiscretizer``_)\n",
    "* This is also called _``BINNING``_ i.e., a grouping of values into 'bins' (e.g. High, Medium, Low)\n",
    "* The discretization transform provides an automatic way to change a numeric input variable to have a different data distribution, which in turn can be used as input to a predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36239b02-5da0-4efe-8fd1-938f14ffe836",
   "metadata": {},
   "source": [
    "#### **Engineer**\n",
    "\n",
    "#### 5.4.1.6.1.7. Polynomial (_``PolynomialFeatures``_)\n",
    "* Polynomial features are those features created by raising existing features to an exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eade7f-4c0d-4ec8-bf7e-5e83b3caa072",
   "metadata": {},
   "source": [
    "#### **5.4.1.6.2. Categorical type**\n",
    "#### **Nominal**\n",
    "\n",
    "#### 5.4.1.6.2.1. One-Hot encode (_``OneHotEncoder``_)\n",
    "* For categorical variables that do not have a natural rank-ordering, i.e., no relationships\n",
    "  ```\n",
    "  Eg., 'red' is (1,0,0), 'green' is (0,1,0), and 'blue' is (0,0,1)\n",
    "  ```\n",
    "* _``LabelEncoder``_ treats class labels as categorical data with no order associated with it\n",
    "* The problem arises when we apply the same approach to transform the nominal variable with _``LabelEncoder``_\n",
    "* For example, in _``LabelEncode``_, the values are encoded as 0, 1, 2 for 'high', 'low', 'medium' respectively. This is OK for ordinal variables but not for nominal variables\n",
    "* Although there is no order involved, a learning algorithm will assume that _``high < low < medium``_. This is a wrong assumption and it will not produce desired results\n",
    "* To fix this issue, a common solution is to use a technique called _``one-hot-encoding``_\n",
    "* In this technique, we create a new dummy feature for each unique value in the nominal feature column. The value of the dummy feature is equal to one when the unique value is present and zero otherwise. Similarly, for another unique value, the value of the dummy feature is equal to one when the unique value is present and zero otherwise. This is called one-hot encoding because only one dummy feature will be equal to one (hot), while the others will be zero (cold)\n",
    "\n",
    "#### 5.4.1.6.2.2. Dummy encode\n",
    "* _``Dummy encoding``_: Avoid redundancy in One-Hot encoding\n",
    "  ```\n",
    "  Eg., 'red' is (1,0), 'green' is (0,1), and 'blue is (0,0)\n",
    "  ```\n",
    "\n",
    "#### 5.4.1.6.2.3. Label binarize (_``LabelBinarizer``_)\n",
    "* We can accomplish two tasks (encoding multi-class labels to integer categories, then from integer categories to one-hot vectors or binary labels) in one shot using the Scikit-Learn’s _``LabelBinarizer``_ class, in other words, it combines _``Label encode + One-hot encode``_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0427b54b-5ca2-4e65-b7b5-2a886387b1b4",
   "metadata": {},
   "source": [
    "#### **Ordinal**\n",
    "\n",
    "#### 5.4.1.6.2.4. Label encode (_``LabelEncoder``_)\n",
    "* The ML algorithms require that class labels are encoded as integers and most estimators for classification convert class labels to integers internally\n",
    "\n",
    "#### 5.4.1.6.2.5. Ordinal encode (_``OrdinalEncoder``_)\n",
    "* For categorical variables that do not have a natural rank/ ordering, i.e., each unique category value is assigned an integer value\n",
    "  ```\n",
    "  Eg., 'red' is 1, 'green' is 2, and 'blue' is 3\n",
    "  ```\n",
    "\n",
    "_**Differences between _``LabelEncoder``_ and _``OrdinalEncoder:``_**\n",
    "```\n",
    "LabelEncoder                            OrdinalEncoder\n",
    "-------------------------------------   --------------------------------------------------\n",
    "- Deals with 1D data, i.e., n_samples   - Deals with 2D data, i.e., n_features, n_samples\n",
    "- Used to encode ‘Target variable’      - Used to encode ‘independent features’\n",
    "```\n",
    "\n",
    "_**IMPORTANT NOTE:**_ \n",
    "* _Apply any feature scaling or transformation technique (such as normalization or standardization etc.) on training & testing datasets separately to prevent DATA LEAKAGE. In other words, ``DO NOT apply data transformation techniques before splitting the datasets into training & testing datasets``_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b826e7c1-491c-47f2-a022-fe804a05aad1",
   "metadata": {},
   "source": [
    "### 5.4.1.7. Handling Imbalanced classes\n",
    "* Any real-world dataset may come with several problems and the imbalanced classes are one of them\n",
    "* The problem of imbalanced classes arises when one set of classes dominates over another set of classes\n",
    "* The former is called the majority class while the latter is called the minority class\n",
    "* This is a very common problem in machine learning where we have datasets with a disproportionate ratio of observations in each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfda041-cbf8-44f4-a39d-4fbd11adb649",
   "metadata": {},
   "source": [
    "#### 5.4.1.7.1. Problems with imbalanced learning\n",
    "* The problem of imbalanced classes is very common and it is bound to happen\n",
    "* This learning from imbalanced data is referred to as imbalanced learning\n",
    "* Significant problems may arise with imbalanced learning. These are as follows:\n",
    "  - It causes the machine learning model to be more biased towards the majority class\n",
    "  - It causes poor classification of minority classes. Hence, this problem throws the question of \"accuracy\" out of the question\n",
    "  - If the Imbalanced classes problem is not addressed properly, then we may end up with higher accuracy. But this higher accuracy is meaningless because it comes from a meaningless metric that is not suitable for the dataset in question. Hence, this higher accuracy no longer reliably measures model performance\n",
    "  - There may be inherent complex characteristics in the dataset. Imbalanced learning from such datasets requires new approaches, principles, tools, and techniques. But it cannot guarantee an efficient solution to the business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713b893-d147-4426-a47f-0edcaf42fe5e",
   "metadata": {},
   "source": [
    "#### 5.4.1.7.2. Example of imbalanced classes\n",
    "* The problem of imbalanced classes may appear in many areas including the following:\n",
    "  - Disease detection, Fraud detection, Anomaly detection\n",
    "  - Earthquake prediction, Churn prediction, Intrusion prediction\n",
    "  - Spam filtering, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be198f-9f2b-47d8-91a0-63fe702ab3e5",
   "metadata": {},
   "source": [
    "#### 5.4.1.7.3. Approaches to handle imbalanced classes\n",
    "There are several methods to deal with the imbalanced class problems and the common ones are listed below\n",
    "* ``Undersampling methods:``\n",
    "The undersampling methods work with the majority class. In these methods, we randomly eliminate instances of the majority class. It reduces the number of observations from the majority class to make the dataset balanced. It results in a severe loss of information. This method is applicable when the dataset is huge and reducing the number of training samples makes the dataset balanced.\n",
    "    1. Random\n",
    "    2. Informative\n",
    "    3. NearMiss\n",
    "    4. Tomek links\n",
    "    5. Edited nearest neighbors\n",
    "    6. Cluster centroids\n",
    "<br><br>\n",
    "* ``Oversampling methods:``\n",
    "The Oversampling methods work with the minority class. In these methods, we duplicate random instances of the minority class. So, it replicates the observations from minority classes to balance the data. It is also known as upsampling. It may result in overfitting due to duplication of data points.\n",
    "    1. Random \n",
    "    2. Cluster-based\n",
    "    3. Synthetic data generation (SMOTE & ADASYN)\n",
    "\n",
    "    _**IMPORTANT NOTE**_: _``SMOTE resampling should NOT be done for test datasets (\"X_test\" & \"y_test\")``_\n",
    "\n",
    "* ``Other methods:``\n",
    "    1. Cost-sensitive learning\n",
    "    2. Algorithmic Ensemble methods\n",
    "    3. Imbalanced learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd135bf3-5499-4505-9468-40b1c601181a",
   "metadata": {},
   "source": [
    "# 5.5. Stage-5: Model Development\n",
    "ML team owns full responsibility for this stage. During this stage, the ML models are developed using the selected list of algorithms at Stage 3 and the pre-processed data at Stage 4. The model could be developed from scratch or fine-tune a pre-trained/available one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd49a41-71ce-4595-a5e4-08f6c437bc60",
   "metadata": {},
   "source": [
    "# 5.6. Stage-6: Model Training\n",
    "ML team owns full responsibility for this stage. During this stage, the _``Evaluation metrics``_ are generated using the _``Training datasets``_. Based on their performance, only a ``few models are selected`` for improvement.\n",
    "\n",
    "There are many ways to split the datasets depending on the size of the dataset to improve model learning. All these methods are used to train, refine and evaluate the ML models.\n",
    "* _K-Fold cross-validation_: \n",
    "  - A resampling procedure to evaluate the performance of the models on a small dataset (high computational cost)\n",
    "  - This procedure should be used directly during the Model Training and initial model selection stage\n",
    "  - Use _``KFold()``_ class from _sklearn.model_selection_\n",
    "  - Usually, k is set to 10 (k=10) for 10 splits (for testing 10 models, each time training with 9 sets & testing with 1 hold-out set)\n",
    "* _StratifiedKFold_: \n",
    "  - Each fold in the split has the same proportion of observations with a given categorical value such as class outcome\n",
    "  - Use _``StratifiedKFold()``_ class from _sklearn.model_selection_\n",
    "* _RepeatedKFold_:\n",
    "  - KFold cross-validation is repeated n times\n",
    "  - Use _``RepeatedKFold()``_ class from _sklearn.model_selection_\n",
    "  - Use this for _**Regression**_ models\n",
    "* _RepeatedStratifiedKFold_:\n",
    "  - It's a combination of Stratified KFold and Repeated KFold procedures\n",
    "  - Use _``RepeatedStratifiedKFold()``_ class from _sklearn.model_selection_\n",
    "  - Use this for _**Classification**_ models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f7ad8f-bf13-4996-8ecd-ffa432236716",
   "metadata": {},
   "source": [
    "# 5.7. Stage-7: Model Refinement\n",
    "ML team owns full responsibility for this stage. During this stage, two main activities are performed:\n",
    "   1. Based on the metrics generated at Stage 6, the models are compared and initial selections are made\n",
    "   2. The selected models are refined to improve their performances using _``Training datasets``_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3372c3-32d2-405f-9b72-a1608368ff63",
   "metadata": {},
   "source": [
    "## 5.7.1. Hyperparameters optimization\n",
    "### 5.7.1.1. Differences between Model Parameters and Model Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038778bf-2db6-4488-9797-58b1e85b1570",
   "metadata": {},
   "source": [
    "![](figures/MLPG-DiffParamsHyperparams.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782cba36-9f24-49b3-a5c1-5f3e76c28fc7",
   "metadata": {},
   "source": [
    "### 5.7.1.2. Hyperparameters-Tuning for Classification Algorithms\n",
    "``A model is a hypothesis and its parameters allow us to tailor the hypothesis (i.e., the behavior of the algorithm) to a specific dataset.``\n",
    "* The more hyperparameters of an algorithm that one needs to tune, the slower the tuning process is. Therefore, it is desirable to select a minimum subset of model hyperparameters to search or tune\n",
    "* Not all model hyperparameters are equally important. Some hyperparameters have an outsized effect on the behavior, and in turn, the performance of an ML algorithm\n",
    "* As an ML practitioner, one must know which hyperparameters to focus on to get a good result quickly\n",
    "\n",
    "The following table summarizes the suggestions for hyperparameters-tuning for 7 Classification algorithms. Please note, that the list of algorithms and the hyperparameters are not exhaustive; these are just examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e35b3-ad9c-47d5-92a3-a790b0bd8c67",
   "metadata": {},
   "source": [
    "![](figures/MLPG-HyperparamsTuning.png)\n",
    "* Hyperparameters Optimization can be done with ``Random Search`` and ``Grid Search``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0d6ec-f13a-4788-90d5-2cb7537579f5",
   "metadata": {},
   "source": [
    "_**IMPORTANT NOTE**_: _Apply any algorithm fine-tuning of hyperparameter technique on training & testing datasets separately to prevent DATA LEAKAGE. In other words, DO NOT apply algorithm fine-tuning techniques before splitting the datasets into training & testing datasets._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7aa117-32c3-496e-babe-ce722af3bb14",
   "metadata": {},
   "source": [
    "# 5.8. Stage-8: Model Evaluation\n",
    "ML team owns full responsibility for this stage. During this stage, the models trained using _``Test datasets``_ are evaluated by comparing the Evaluation metrics generated for each model (eg., Accuracy score).\n",
    "\n",
    "_**NOTE:**_\n",
    "* _The _``evaluation Metrics``_ should be the same across all stages so that the comparisons can be made and models can be selected_\n",
    "* _One way to compare the models is to generate bar charts with Accuracy/ROC-AUC/CV scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec3ca8-5a3f-496f-91c7-86916b04677a",
   "metadata": {},
   "source": [
    "# 5.9. Stage-9: Final Model Selection\n",
    "Based on the _``evaluation metrics``_ generated at Stage 7 (on training datasets) and Stage 8 (on test datasets), the final model is selected. The final model is trained on the entire dataset, saved, and deployed into the test infrastructure for business user testing (i.e., for Model Validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac5b27d-fa24-4bd2-857f-4c70bcacef4c",
   "metadata": {},
   "source": [
    "# 5.10. Stage-10: Model Validation\n",
    "The business users perform the validations using the _``unseen UAT datasets``_.\n",
    "\n",
    "_**NOTE:**_ \n",
    "* _The Evaluation Metrics and Testing metrics should be the same_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705ca7a-2119-4a13-b9b6-d4d4a1ec89bf",
   "metadata": {},
   "source": [
    "# 5.11. Stage-11: Model Deployment\n",
    "The model deployment into the production environment is usually done by the DS team along with the ML team. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d47a88-0396-44c5-bbfe-21157a8224ac",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<br>\n",
    "\n",
    "<[ [Machine Learning Project – Process Definition](04.00-mlpgw-Machine-Learning-Project–Process-Definition.ipynb) | [Contents and Acronyms](00.00-mlpgw-Contents-and-Acronyms.ipynb) | [Other Considerations - How to choose ML Algorithms](06.01-mlpgw-Other-Considerations-How-to-choose-ML-Algorithms.ipynb) ]>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
