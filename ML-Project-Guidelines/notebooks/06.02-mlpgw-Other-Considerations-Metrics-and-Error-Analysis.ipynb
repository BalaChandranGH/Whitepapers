{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36f4fcb-9542-4ae6-9419-5afd2aae21d8",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-Book-Cover-Small.png\"><br>\n",
    "\n",
    "This notebook contains an excerpt from the **`Machine Learning Project Guidelines - For Beginners`** whitepaper written by *Balasubramanian Chandran*; the content is available [on GitHub](https://github.com/BalaChandranGH/Whitepapers/ML-Project-Guidelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b6351-c01f-4f7e-86c1-2e3a069e4027",
   "metadata": {},
   "source": [
    "<br>\n",
    "<!--NAVIGATION-->\n",
    "\n",
    "<[ [Other Considerations - How to choose ML Algorithms](06.01-mlpgw-Other-Considerations-How-to-choose-ML-Algorithms.ipynb) | [Contents and Acronyms](00.00-mlpgw-Contents-and-Acronyms.ipynb) | [Other Considerations - Model Performance Improvement](06.03-mlpgw-Other-Considerations-Model-Performance-Improvement.ipynb) ]>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f419b-59a4-486e-9295-fde96b48153a",
   "metadata": {},
   "source": [
    "# 6. Other Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7840258e-0f9b-4364-89f4-c64b325cd51a",
   "metadata": {},
   "source": [
    "## 6.2. Metrics and Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886a76e-b7ed-4251-be90-60715813d045",
   "metadata": {},
   "source": [
    "### 6.2.1. Learning Curves\n",
    "* Generally, a learning curve is a plot that shows time or experience on the x-axis and learning or improvement on the y-axis\n",
    "* Learning curves are widely used in machine learning for algorithms that learn (optimize their internal parameters) incrementally over time, such as deep learning neural networks\n",
    "* The metric used to evaluate learning could be maximizing, meaning that better scores (larger numbers) indicate more learning. An example would be classification accuracy\n",
    "* It is more common to use a score that is minimizing, such as loss or error whereby better scores (smaller numbers) indicate more learning and a value of 0.0 indicates that the training dataset was learned perfectly and no mistakes were made\n",
    "* There are three common dynamics that you are likely to observe in learning curves; they are:\n",
    "  - Underfit\n",
    "  - Overfit\n",
    "  - Good Fit\n",
    "* Most commonly, learning curves are used to diagnose the overfitting behavior of a model that can be addressed by tuning the hyperparameters of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f21fc-e1ad-4281-ade0-ccdbfc7c1225",
   "metadata": {},
   "source": [
    "### 6.2.2. Precision and Recall\n",
    "* Both are threshold metrics for classification problems (for both balanced and Imbalanced)\n",
    "* Precision attempts to answer “What proportion of positive identifications was actually correct?”\n",
    "* Recall attempts to answer “What proportion of actual positives was identified correctly?”\n",
    "* A **false positive** is an incorrect identification of the presence of a condition when it’s absent\n",
    "* A **false negative** is an incorrect identification of the absence of a condition when it’s actually present\n",
    "* Examples of when to give importance to false positives and false negatives:\n",
    "  - Consider a case when a country wants to vaccinate all its people aged over 50\n",
    "    - ``False positives are more important`` if the policy is not to leave even a single person over the age of 50 and it’s ok to vaccinate up to 5% of its people below the age of 50 as well if they are wrongly identified because the country has 5% more vaccinations as a contingency\n",
    "    - ``False negatives are more important`` if the policy is not to allow even a single person below the age of 50 because of the shortage of vaccinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19416f67-07d4-484b-939b-6db7f11e667b",
   "metadata": {},
   "source": [
    "### 6.2.3. Performing Error Analysis/Troubleshooting Prediction Errors\n",
    "* **Error analysis**: The **``recommended approach``** to solving machine learning problems is,\n",
    "  - Start with a simple algorithm, implement it quickly, and test it early on the cross-validation dataset\n",
    "  - Plot learning curves to decide if more data, more features, etc., are likely to help\n",
    "  - Manually examine the errors on examples in the cross-validation dataset and try to spot a trend where most of the errors were made\n",
    "* Errors in predictions or evaluating a hypothesis can be troubleshot by the following, to name a few.\n",
    "  - Getting more training examples\n",
    "  - Trying smaller sets of features\n",
    "  - Trying additional features\n",
    "  - Trying polynomial features\n",
    "  - Regularization methods (increasing or decreasing “lambda”)\n",
    "* Don't just pick one of these avenues at random, the diagnostic techniques for choosing one of the above solutions are discussed below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b0128-3204-40a2-a397-934b543add20",
   "metadata": {},
   "source": [
    "**1) Model selection and ``Train-Validation-Test datasets``**\n",
    "  - Split the dataset into 3 datasets (the % can differ):\n",
    "  ```\n",
    "    - Training set         - 60%\n",
    "    - Cross-validation set - 20%\n",
    "    - Test set             - 20%\n",
    "  ```\n",
    "  - Now, calculate 3 separate error values for the 3 different datasets using the following method\n",
    "  - Optimize the parameters in **``theta``** using the training set for each polynomial degree\n",
    "  - Find the polynomial degree **``d``** with the least error using the cross-validation set\n",
    "  - Estimate the generalization error using the test set with d (note, **``d``** is not trained using the test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa81896-22be-4b02-b224-26fd50234c6e",
   "metadata": {},
   "source": [
    "**2) Diagnosing Bias vs Variance – ``Model complexity``** (lower-order/higher-order polynomial **d** contribution)\n",
    "  - We need to distinguish whether ``bias`` or ``variance`` is the problem contributing to the bad predictions\n",
    "  - ``High bias`` means ``underfitting`` and ``high variance`` means ``overfitting``\n",
    "  - Ideally, we need to find a golden mean (sweet-spot) between bias and variance (trade-off)\n",
    "  - The ``training error`` will tend to ``decrease`` as we increase the degree **d** of the polynomial, at the same time the ``cross-validation error`` will tend to ``decrease`` as we increase **d** up to a point, then it will ``increase`` as **d** is increased, forming a convex curve as depicted below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46e45b-430f-4b1b-9949-8ca7a5b06229",
   "metadata": {},
   "source": [
    "**3) Diagnosing and Bias-Variance –** **``Regularization``** ``(parameter``**``lambda``**)\n",
    "  - ``Large lambda`` means ``high bias (underfitting)``\n",
    "  - ``Low lambda`` means ``high variance (overfitting)``\n",
    "  - ``Intermediate lambda`` means ``right-fit or just-right``<br>\n",
    "  The figure below illustrates the relationship between lambda and the hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046dbfa0-dfb3-4386-b79e-caca49ccefce",
   "metadata": {},
   "source": [
    "![](figures/MLPG-OC-ModelComplexity.png) ![](figures/MLPG-OC-Regularizattion.png) <br>\n",
    "Image credit [ (Source) ](https://www.coursera.org/in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c08638-4802-4c9b-a74c-c1307cd57c0f",
   "metadata": {},
   "source": [
    "**4) Learning curves**\n",
    "  - Training an algorithm on a very few numbers of data points will easily have 0 errors because we can always find a quadradic curve that touches exactly those number of points, hence:\n",
    "    - As the training set gets larger, the error for the quadradic function increases\n",
    "    - The error value will plateau out after a certain m or training set size\n",
    "  - **Experiencing high bias:**\n",
    "    - **``Low training set size``**: causes ``low train error`` and ``high CV error``\n",
    "    - **``Large training set size``**: causes both ``train error`` and ``CV error high``\n",
    "    - If a learning algorithm is suffering from ``high bias, getting more training data will not (by itself) help much``\n",
    "  - **Experiencing high variance:**\n",
    "    - **Low training set size**: causes ``low train error`` and ``high CV error``\n",
    "    - **Large training set size**: train error increases with the training set and CV error continues to decrease without leveling off, also, the train error < CV error, but the difference between them is significant\n",
    "    - If a learning algorithm is suffering from ``high variance, getting more training data likely to help``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf806e63-2d4e-423e-8fa8-3db9c9684d13",
   "metadata": {},
   "source": [
    "![](figures/MLPG-OC-BiasVsVariance.png)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Image credit [ (Source) ](https://www.coursera.org/in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4698d8-85b8-4f5b-a799-9ae87a60fbc0",
   "metadata": {},
   "source": [
    "**5) Diagnosing Neural Networks (NN)**\n",
    "  - A NN with ``fewer parameters`` is ``prone to underfitting`` but computationally cheaper\n",
    "  - A large NN with ``more parameters`` is ``prone to overfitting`` and computationally expensive\n",
    "  - Using a single hidden layer is a good starting default\n",
    "  - We can train our NN on many hidden layers using the CV set, then select the one that performs best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a7fefb-f3d8-42b9-8a16-53e63f76462c",
   "metadata": {},
   "source": [
    "**In SUMMARY - Deciding what to do next**\n",
    "* Model complexity effects\n",
    "  - Low model complexity results in  ``High Bias``, but ``Low Variance`` (lower-order polynomials)\n",
    "  - High model complexity results in ``High Variance``, but ``Low Bias`` (higher-order polynomials)\n",
    "* A **``rule of thumb``** when running diagnostics is:\n",
    "  - Getting more training examples: ``Fixes High Variance``, but ``not High Bias``\n",
    "  - Trying fewer features: ``Fixes High Variance``, but ``not High Bias``\n",
    "  - Adding features: ``Fixes High Bias``, but ``not High Variance`` \n",
    "  - Adding polynomial features: ``Fixes High Bias``, but ``not High Variance`` \n",
    "  - Decreasing lambda: ``Fixes High Bias``\n",
    "  - Increasing lambda: ``Fixes High Variance``\n",
    "  - Smaller NN: prone to ``Underfitting``\n",
    "  - Larger NN: prone to ``Overfitting``\n",
    "\n",
    "**NOTE:**\n",
    "* Adding features or adding polynomial features means increasing the complexity of the model\n",
    "* Lower-order polynomials mean low model complexity\n",
    "* Higher-order polynomials mean high model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490b088-098d-4d11-8047-4eca899708bf",
   "metadata": {},
   "source": [
    "### 6.2.4. How to evaluate and select ML models in Supervised Learning?\n",
    "* There are a variety of evaluation metrics to evaluate supervised ML models for,\n",
    "  - Classification problems, and \n",
    "  - Regression problems\n",
    "* Choose the right metric for selecting between models and/or for doing parameter tuning\n",
    "* It's very important to choose evaluation methods that match the goal of the application\n",
    "* Compute the selected evaluation metric for multiple different models\n",
    "* Then select the model with the \"best\" value of the evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b4424c-63ba-4bf3-9452-eff19cb61df6",
   "metadata": {},
   "source": [
    "**Basic Evaluation Metrics (for ``Classification``):**\n",
    "* Different applications have different goals, Accuracy is widely used, but many others are possible\n",
    "* The metric Accuracy gives only a partial picture of a classifier's performance\n",
    "* Dummy classifiers:\n",
    "  - Dummy classifiers completely ignore the input data!\n",
    "  - They provide a null-numeric (e.g., null accuracy) baseline\n",
    "  - ``Dummy classifiers should not be used for real problems``\n",
    "* **``Confusion Matrix``** for binary prediction task:\n",
    "\n",
    "![](figures/MLPG-OC-ConfusionMatrix.png)\n",
    "  - **``Rule of thumb``**: as part of model evaluation, always look at the confusion matrix for the classifier\n",
    "  - To get some insight into what kind of errors it is making for each class including whether some classes are much more prone to certain kinds of errors than others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4701df01-a44b-426e-a1ef-37390c2df0c7",
   "metadata": {},
   "source": [
    "* ``Accuracy``: for what fraction of all instances is the classifier's prediction ``correct`` (for either +ve or -ve class)?<br> \n",
    "    $Accuracy = \\frac{TN\\ +\\ TP}{TN\\ +\\ TP\\ +\\ FN\\ +\\ FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12380ed5-e2fb-479a-b75d-af452c7a7124",
   "metadata": {},
   "source": [
    "* ``Classification error``: for what fraction of all instances is the classifier's prediction ``incorrect``?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c97d2-bb83-499f-8f2e-937558013c17",
   "metadata": {},
   "source": [
    "* ``Precision``: what fraction of ``positive`` predictions are correct?<br>\n",
    "$Precision = \\frac{TP}{TP\\ +\\ FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6774a045-cd89-45ea-91a7-ef139729a691",
   "metadata": {},
   "source": [
    "* ``Recall (aka TPR)``: what fraction of all ``positive`` instances does the classifier ``correctly`` identify as positive?<br>\n",
    "$Recall\\ [aka\\ True\\ Positve\\ Rate\\ (TPR)] = \\frac{TP}{(TP\\ +\\ FN)}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43130f9-be1b-4a7e-9aa8-7d9ad33bd2ce",
   "metadata": {},
   "source": [
    "* ``FPR``: what fraction of all ``negative`` instances does the classifier ``incorrectly`` identify as positive?<br>\n",
    "$False\\ Positive\\  Rate\\ (FPR) = \\frac{FP}{FP\\ +\\ TN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e7d52-bc55-4e94-ac95-2e0526e898f9",
   "metadata": {},
   "source": [
    "* ``F1-Score``: combines Precision and Recall into a single number<br>\n",
    "$F1_{Score} = 2 * \\frac{Precision\\ *\\ Recall}{Precision\\ +\\ Recall}   =  \\frac{2\\ *\\ TP}{2\\ *\\ TP\\ +\\ FN\\ +\\ FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ababb1-fe2e-4d7c-84aa-139c2277fd7d",
   "metadata": {},
   "source": [
    "* There is often a trade-off between Precision and Recall\n",
    "  - Recall-oriented ML tasks\n",
    "    - Tumor detection\n",
    "    - Search and information extraction in legal discovery\n",
    "    - Often paired with a human expert to filter out false positives\n",
    "  - Precision-oriented ML tasks\n",
    "    - Search engine ranking, query suggestions\n",
    "    - Document classification\n",
    "    - Many customer-facing tasks (users remember failures!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c4184-acc0-4502-8db4-219db5f47925",
   "metadata": {},
   "source": [
    "**Predicted Probability of Class Membership (predict_proba):**\n",
    "* Typical rule: Choose most likely class (e.g., Class 1 if threshold > 0.50)\n",
    "* The adjusting threshold affects the predictions of the classifier\n",
    "* Higher threshold results in a more conservative classifier\n",
    "* Not all models provide realistic probability estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88822c4c-7982-480e-b088-40ca6e24c44f",
   "metadata": {},
   "source": [
    "![](figures\\MLPG-OC-PRCurves.png) \n",
    "![](figures\\MLPG-OC-ROCCurves.png)<br>\n",
    "Image Credit [ (Source) ](https://www.coursera.org/in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5cba0b-0b52-4bea-b403-f3f81436c39f",
   "metadata": {},
   "source": [
    "**Basic Evaluation Metrics (for ``Regression``):**\n",
    "* Typically, _**``r2_score``**_ is enough\n",
    "  - Computes how well the future instances will be predicted\n",
    "  - The best possible score is **1.0**\n",
    "  - The constant prediction score is **0.0**\n",
    "* Alternative metrics include:\n",
    "  - _**``MAE``**_: ``Mean Absolute Error`` (absolute difference of target & predicted values)\n",
    "  - _**``MSE``**_: ``Mean Squared Error`` (squared difference of target & predicted values)\n",
    "  - ``Median Absolute Error`` (robust to outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e51d73-0668-43a9-ba79-2e13193beeaf",
   "metadata": {},
   "source": [
    "**Model Selection: Optimizing Classifiers for different evaluation metrics:**\n",
    "* Training, Validation, and Test framework for model evaluation and selection\n",
    "  - Using only CV or test set to do model selection may lead to more subtle overfitting\n",
    "  - Instead, use 3 data splits:\n",
    "    - Training set (for model building)\n",
    "    - Validation set (for initial model selection)\n",
    "    - Test set (for final model evaluation & selection)\n",
    "  - In practice:\n",
    "    - ``Create an initial train/test split``\n",
    "    - ``Do CV on the training set for initial model/parameter selection``\n",
    "    - ``Save the held-out test set for final model evaluation & selection``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54b81a4-9d7c-4d85-91fb-2343122de2c5",
   "metadata": {},
   "source": [
    "* ``Accuracy`` is often not the right evaluation metric for many real-world ML tasks\n",
    "  - False positives and False negatives may need to be treated very differently\n",
    "  - Make sure we understand the needs of our application and choose an evaluation metric that matches our application, user, and business goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c49bfd-aa04-42ed-b226-12661b83f290",
   "metadata": {},
   "source": [
    "* Additional evaluation methods include (_examples_):\n",
    "  - ``Learning curve``\n",
    "    - How much does accuracy/other metrics change as a function of the amount of training data?\n",
    "  - ``Sensitivity analysis``\n",
    "    - How much does accuracy/other metrics change as a function of key learning parameter values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d47a88-0396-44c5-bbfe-21157a8224ac",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<br>\n",
    "\n",
    "<[ [Other Considerations - How to choose ML Algorithms](06.01-mlpgw-Other-Considerations-How-to-choose-ML-Algorithms.ipynb) | [Contents and Acronyms](00.00-mlpgw-Contents-and-Acronyms.ipynb) | [Other Considerations - Model Performance Improvement](06.03-mlpgw-Other-Considerations-Model-Performance-Improvement.ipynb) ]>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
